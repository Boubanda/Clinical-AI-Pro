"""
Suite IA avanc√©e pour pr√©diction clinique - VERSION FINALE
Utilise XGBoost pour capturer les patterns cliniques
Sortie optimis√©e pour dashboard Streamlit
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from xgboost import XGBClassifier
import warnings
warnings.filterwarnings('ignore')

print("ü§ñ Lancement de la suite IA Clinical Analytics...")
print("="*60)

# Charger donn√©es
df = pd.read_csv('clinical_data_realistic_test.csv')
print(f"üìä Donn√©es charg√©es: {len(df)} patients\n")

# ============= MOD√àLE 1: PR√âDICTION DROPOUT =============
print("üìä MOD√àLE 1: Pr√©diction risque d'abandon patient (XGBoost)")
print("-" * 50)

# Features pour pr√©diction
features_dropout = ['age', 'data_entry_delay_days', 'query_rate', 
                   'visit_compliance', 'protocol_deviations', 'risk_score']
X = df[features_dropout].copy()
y = df['is_dropout'].astype(int)

print(f"üìà Distribution dropout: {y.sum()} positifs / {len(y)} total ({y.mean()*100:.1f}%)")

# Division train/test (stratifi√© pour √©quilibrer)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# XGBoost Model
xgb_model = XGBClassifier(
    n_estimators=150,
    max_depth=6,
    learning_rate=0.05,
    random_state=42,
    eval_metric='logloss'
)
xgb_model.fit(X_train, y_train, verbose=False)

# Pr√©dictions
y_pred = xgb_model.predict(X_test)
y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]
df['dropout_prediction'] = xgb_model.predict_proba(X)[:, 1]

# M√©triques
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, zero_division=0)
recall = recall_score(y_test, y_pred, zero_division=0)
auc_score = roc_auc_score(y_test, y_pred_proba)

high_risk_patients = (df['dropout_prediction'] > 0.6).sum()
very_high_risk = (df['dropout_prediction'] > 0.8).sum()

print(f"‚ö†Ô∏è  Patients √† risque moyen (>0.6): {high_risk_patients}")
print(f"üî¥ Patients √† tr√®s haut risque (>0.8): {very_high_risk}")
print(f"\nüìä M√©triques mod√®le:")
print(f"   Accuracy: {accuracy:.1%} | Precision: {precision:.1%} | Recall: {recall:.1%}")
print(f"   AUC-ROC: {auc_score:.3f}")

# Feature importance
feature_importance = pd.DataFrame({
    'feature': features_dropout,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nüéØ Features les plus pr√©dictives:")
for idx, row in feature_importance.head(3).iterrows():
    print(f"   {idx+1}. {row['feature']}: {row['importance']:.3f}")

# ============= MOD√àLE 2: D√âTECTION ANOMALIES =============
print("\nüîç MOD√àLE 2: D√©tection d'anomalies multicrit√®res")
print("-" * 50)

# Features pour anomalies
features_anomaly = ['current_hemoglobin', 'current_creatinine', 'current_alt', 
                   'query_rate', 'data_entry_delay_days', 'protocol_deviations']
X_anomaly = df[features_anomaly].copy()

# Normalisation
scaler = StandardScaler()
X_anomaly_scaled = scaler.fit_transform(X_anomaly)

# Isolation Forest
iso_model = IsolationForest(contamination=0.05, random_state=42)
df['anomaly_flag'] = iso_model.fit_predict(X_anomaly_scaled)
df['anomaly_detected'] = df['anomaly_flag'].apply(lambda x: 'Anomalie' if x == -1 else 'Normal')

n_anomalies = (df['anomaly_flag'] == -1).sum()
print(f"üö® Anomalies d√©tect√©es: {n_anomalies} patients ({n_anomalies/len(df)*100:.1f}%)")

# Analyse par site
print(f"\nüìç R√©partition anomalies par site:")
anomaly_by_site = df[df['anomaly_flag'] == -1].groupby('site_name').size().sort_values(ascending=False)
for site, count in anomaly_by_site.items():
    site_total = len(df[df['site_name'] == site])
    pct = count / site_total * 100
    print(f"   {site}: {count} anomalies ({pct:.1f}% du site)")

# ============= MOD√àLE 3: CLUSTERING SITES =============
print("\nüéØ MOD√àLE 3: Segmentation intelligente des sites")
print("-" * 50)

# Agr√©gation par site
site_metrics = df.groupby('site_name').agg({
    'query_rate': 'mean',
    'tmf_completeness_%': 'mean',
    'protocol_deviations': 'sum',
    'data_entry_delay_days': 'mean',
    'dropout_prediction': 'mean',
    'is_dropout': 'mean',
    'visit_compliance': 'mean'
}).reset_index()

site_metrics['n_patients'] = df.groupby('site_name').size().values
site_metrics = site_metrics.rename(columns={'site_name': 'site_id'})

# Normalisation pour clustering
X_cluster = scaler.fit_transform(site_metrics.iloc[:, 1:].select_dtypes(np.number))

# K-Means clustering
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
site_metrics['cluster'] = kmeans.fit_predict(X_cluster)

# Mapping bas√© sur performance
cluster_perf = site_metrics.groupby('cluster')['tmf_completeness_%'].mean().sort_values(ascending=False)
cluster_map = {
    cluster_perf.index[0]: '‚≠ê Top Performer',
    cluster_perf.index[1]: 'üìä Standard',
    cluster_perf.index[2]: '‚ö†Ô∏è √Ä Am√©liorer'
}
site_metrics['performance'] = site_metrics['cluster'].map(cluster_map)

print("üìà Classification des sites:")
for _, row in site_metrics.iterrows():
    print(f"\n   {row['site_id']}: {row['performance']}")
    print(f"      Patients: {row['n_patients']:.0f} | TMF: {row['tmf_completeness_%']:.1f}% | Dropout: {row['is_dropout']:.1%}")
    print(f"      Query rate: {row['query_rate']:.2%} | Compliance: {row['visit_compliance']:.1%}")

# ============= CALCUL M√âTRIQUES GLOBALES =============
print("\nüìä M√âTRIQUES GLOBALES")
print("-" * 50)

total_deviations = df['protocol_deviations'].sum()
avg_query_rate = df['query_rate'].mean()
compliance_rate = df['visit_compliance'].mean()
total_dropouts = df['is_dropout'].sum()
dropout_pct = (total_dropouts / len(df)) * 100

print(f"üìã D√©viations protocolaires totales: {total_deviations:.0f}")
print(f"üìù Taux de queries moyen: {avg_query_rate:.2%}")
print(f"‚úÖ Taux de compliance moyen: {compliance_rate:.1%}")
print(f"üö® Dropouts observ√©s: {total_dropouts} / {len(df)} ({dropout_pct:.1f}%)")

# ============= PR√âPARATION POUR LE DASHBOARD =============
# Ajouter colonne site_category pour le dashboard
df['site_category'] = 'Center'

# Ajouter des colonnes de r√©sum√© utiles
df['dropout_risk_level'] = df['dropout_prediction'].apply(
    lambda x: 'Tr√®s Haut Risque' if x > 0.8 else ('Haut Risque' if x > 0.6 else 'Risque Mod√©r√©' if x > 0.4 else 'Bas Risque'))

# ============= EXPORT R√âSULTATS =============
df.to_csv('clinical_predictions.csv', index=False)
site_metrics.to_csv('site_performance.csv', index=False)

print(f"\n‚úÖ Fichiers sauvegard√©s:")
print(f"   ‚Ä¢ clinical_predictions.csv ({len(df)} lignes)")
print(f"   ‚Ä¢ site_performance.csv ({len(site_metrics)} sites)")

print("\n" + "="*60)
print("‚úÖ ANALYSE IA TERMIN√âE")
print("="*60)
print(f"""
üìä R√©sum√© ex√©cutif:
   ‚Ä¢ Patients analys√©s: {len(df)}
   ‚Ä¢ Mod√®les entra√Æn√©s: 3 (XGBoost + IsolationForest + K-Means)
   ‚Ä¢ Accuracy dropout: {accuracy:.1%}
   ‚Ä¢ AUC-ROC: {auc_score:.3f}
   ‚Ä¢ Anomalies d√©tect√©es: {n_anomalies}
   ‚Ä¢ Sites √† am√©liorer: {(site_metrics['performance'] == '‚ö†Ô∏è √Ä Am√©liorer').sum()}
   
üí° Prochaine √©tape: streamlit run 3_dashboard_executive.py
""")